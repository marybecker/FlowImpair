library('RSQLite')
library('stringr')
csv_to_db_datetime<-function(Date_Time){
#comes in in the form: "M||M/DD/YYYY HH:MM" -> YYYY-MM-DD HH:MM:SS
csv_datetime <- as.character(Date_Time);
csv_datetime <- strsplit(csv_datetime,' '); # first split by the whitespace between YYYY and HH
csv_date  <- csv_datetime[[1]][1];
csv_date  <- strsplit(csv_date,'/'); #now split by / for D/M/Y
dates <- c(csv_date[[1]][3],csv_date[[1]][1],csv_date[[1]][2]);
dates <- str_pad(dates,2,pad='0');
csv_time   <- csv_datetime[[1]][2];
csv_time   <- strsplit(csv_time,':'); #now split
times <- c(csv_time[[1]][1],csv_time[[1]][2]);
times <- str_pad(times,2,pad='0');
sql_date_time <- paste(paste(dates[1],dates[2],dates[3],sep='-'),paste(times[1],times[2],sep=':'));
sql_date_time <- paste(sql_date_time,':00',sep='');
sql_date_time #return YY-MM-DD HH:MM:SS
}
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB'
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/'
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/FilesForDB_2016_Spring/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/FilesForDB_2016_Spring/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
SQL <- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
WHERE ProbeID == '9937222'"
response <- dbGetQuery(conn=db,SQL); #returns data.frame
response
SQL <- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
WHERE ProbeID == '9715474'"
response <- dbGetQuery(conn=db,SQL); #returns data.frame
response
SQL<-"SELECT SID, ProbeID,COUNT(ProbeID)
FROM probe_temps
GROUP BY SID,ProbeID"
CntSIDPrb<- dbGetQuery(conn=db,SQL);
CntSIDPrb
SQL<-"SELECT SID, ProbeID
FROM probe_temps
GROUP BY SID,ProbeID"
CntSIDPrb<- dbGetQuery(conn=db,SQL);
CntSIDPrb
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/FilesForDB_2016_Fall/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
dbDisconnect(db);
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/'
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
#Identify and Cnt the Rows Before Upload##############
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/Uploaded/FilesForDB_2016_Spring/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
dbDisconnect(db);
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/'
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
#Identify and Cnt the Rows Before Upload##############
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/Uploaded/FilesForDB_2016_Fall/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
dbDisconnect(db);
library(RSQLite)
library(plyr)
library(ggplot2)
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/' #on windows like this
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
names  <- dbListTables(db);                        # The tables in the database
fields <- dbListFields(db, "probe_temps");    # The columns in a table
table  <- dbReadTable(db, "probe_temps");  # get the whole table as a data.frame
table$day <- substr(table$Date_Time,6,10)##Add column of data that includes month_day
table$month<- substr(table$Date_Time,6,7)##Add column of data that includes month
table$year<- substr(table$Date_Time,1,4)##Add column of data that includes year
AvgDay <- ddply(table,c("ProbeID","SID","day","month","year","Collector","UOM"),summarize,mean=mean(Temp),min=min(Temp),
max=max(Temp),maxmin= (max(Temp)-min(Temp)),N=length(Temp))#AvgByDay
AvgDay$Flag [AvgDay$N<24|AvgDay$min<0|AvgDay$maxmin>5]<-1
write.csv(AvgDay,"S:/M_Kozlak/Temperature/TemperatureDB/Summary_DB/TempAvgDay.csv",row.names=FALSE)
windows(title="Daily Temp Flux By Month",5,5)
ggplot(AvgDay,aes(x=month,y=maxmin,fill=month))+
geom_boxplot()+
ylim(NA,10)
AvgDayMonth<-subset(AvgDay,month=="10")
quantile(AvgDayMonth$maxmin,c(.25,.50,.75,.90,.99))
yr <- 2015 #Specify the year for metric calcs
temp <- subset(table,year==yr)##Subset by Year
AvgDay <- ddply(temp,c("SID","day","month","year"),summarize,mean=mean(Temp),N=length(Temp))#AvgByDay
SummerMonths <- subset(AvgDay,month %in% c('06','07','08')& N==24)#Subset Summer Months
AvgSummerTemp <- ddply(SummerMonths,"SID",summarize,SummerTemp=mean(mean),SN=length(mean))#Summer Temp Month
AvgSummerTemp$TempCatS <- AvgSummerTemp$SummerTemp
AvgSummerTemp$TempCatS<- as.character(AvgSummerTemp$TempCat)
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp<18.29]<-"Cold"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>21.7]<-"Warm"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>=18.29&AvgSummerTemp$SummerTemp<=21.7]<-"Cool"
July <- subset(AvgDay,month %in% '07'& N==24)#Subset
AvgJulyTemp <- ddply(July,"SID",summarize,JulyTemp=mean(mean),JN=length(mean))#Summer Temp Month
AvgJulyTemp$TempCatJ <- AvgJulyTemp$JulyTemp
AvgJulyTemp$TempCatJ<- as.character(AvgJulyTemp$TempCat)
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp<18.45]<-"Cold"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>22.30]<-"Warm"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>=18.45&AvgJulyTemp$JulyTemp<=22.30]<-"Cool"
MaxDailyTemp <- ddply(AvgDay,"SID",summarize,MaxD = max(mean),MN=length(mean))
MaxDailyTemp$TempCatM <- MaxDailyTemp$MaxD
MaxDailyTemp$TempCatM <- as.character(MaxDailyTemp$MaxD)
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD<22.4]<- "Cold"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>26.3]<- "Warm"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>=22.4&MaxDailyTemp$MaxD<=26.3]<- "Cool"
TempMetrics <- merge(AvgSummerTemp,AvgJulyTemp,by="SID")
TempMetrics <-merge(TempMetrics,MaxDailyTemp,by="SID")
TempMetrics$Flag [TempMetrics$SN<92|TempMetrics$JN <31] <- 1
write.csv(TempMetrics,paste("S:/M_Kozlak/Temperature/TemperatureDB/MetricCalcs/TempMetrics",yr,".csv"),row.names=FALSE)
yr <- 2016 #Specify the year for metric calcs
temp <- subset(table,year==yr)##Subset by Year
AvgDay <- ddply(temp,c("SID","day","month","year"),summarize,mean=mean(Temp),N=length(Temp))#AvgByDay
##Avg Summer Temp##
SummerMonths <- subset(AvgDay,month %in% c('06','07','08')& N==24)#Subset Summer Months
AvgSummerTemp <- ddply(SummerMonths,"SID",summarize,SummerTemp=mean(mean),SN=length(mean))#Summer Temp Month
AvgSummerTemp$TempCatS <- AvgSummerTemp$SummerTemp
AvgSummerTemp$TempCatS<- as.character(AvgSummerTemp$TempCat)
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp<18.29]<-"Cold"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>21.7]<-"Warm"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>=18.29&AvgSummerTemp$SummerTemp<=21.7]<-"Cool"
##Avg July Temp##
July <- subset(AvgDay,month %in% '07'& N==24)#Subset
AvgJulyTemp <- ddply(July,"SID",summarize,JulyTemp=mean(mean),JN=length(mean))#Summer Temp Month
AvgJulyTemp$TempCatJ <- AvgJulyTemp$JulyTemp
AvgJulyTemp$TempCatJ<- as.character(AvgJulyTemp$TempCat)
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp<18.45]<-"Cold"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>22.30]<-"Warm"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>=18.45&AvgJulyTemp$JulyTemp<=22.30]<-"Cool"
##Max Daily Mean##
MaxDailyTemp <- ddply(AvgDay,"SID",summarize,MaxD = max(mean),MN=length(mean))
MaxDailyTemp$TempCatM <- MaxDailyTemp$MaxD
MaxDailyTemp$TempCatM <- as.character(MaxDailyTemp$MaxD)
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD<22.4]<- "Cold"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>26.3]<- "Warm"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>=22.4&MaxDailyTemp$MaxD<=26.3]<- "Cool"
##Combine and Export Metrics By Year
TempMetrics <- merge(AvgSummerTemp,AvgJulyTemp,by="SID")
TempMetrics <-merge(TempMetrics,MaxDailyTemp,by="SID")
TempMetrics$Flag [TempMetrics$SN<92|TempMetrics$JN <31] <- 1
write.csv(TempMetrics,paste("S:/M_Kozlak/Temperature/TemperatureDB/MetricCalcs/TempMetrics",yr,".csv"),row.names=FALSE)
dbDisconnect(db);
library(data.table)
library(ggplot2)
library(lubridate)
library(reshape2)
setwd("P:/Projects/GitHub_Prj/FlowImpair")
indexgage<-read.csv("usgsindexgage.csv",header=TRUE)
indexgage$SiteNumber<-paste("0",indexgage$SiteNumber,sep="")
#assume one header line and tab delimited structure, with # as a comment out to skip
parse_fstat<-function(fstat_lines,skip='#',delim='\t'){
x<-1;
while(x<length(fstat_lines) && startsWith(fstat_lines[x],skip)){
x<-x+1;
}
header<-strsplit(fstat_lines[x],delim)[[1]];
D<-as.data.frame(matrix('',ncol=length(header),nrow=length(fstat_lines)-x),stringsAsFactors=F);
colnames(D)<-header;
for(i in x+2:length(fstat_lines)){
r<-strsplit(fstat_lines[i],delim)[[1]];
D[i-x-1,1:length(r)]<-r;
}
D
}
#character string from right
substrRight <- function(x, n){
substr(x, nchar(x)-n+1, nchar(x))
}
#build one site flow fstats index
get_sfindex<-function(site,base_url,start_date,end_date,parameterCd){
flow_parts <- c(base_url,'/dv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&statCd=',      '00003',
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
#flow_url<-'https://waterservices.usgs.gov/nwis/dv/?format=rdb&sites=01187300&startDT=1997-01-01&endDT=2017-05-01&statCd=00003&parameterCd=00060&siteType=ST&siteStatus=all'
flow_url<-paste(flow_parts,sep='',collapse='');
flow <- fread(flow_url);
fstat_parts <- c(base_url,'/stat/?format=rdb,1.0',
'&sites=',      site,
'&statReportType=','daily',
'&statTypeCd=',      'all',
'&parameterCd=',   parameterCd);
#fstat_url<- 'https://waterservices.usgs.gov/nwis/stat/?format=rdb,1.0&sites=01118300&statReportType=daily&statTypeCd=all&parameterCd=00060';
fstat_url<-paste(fstat_parts,sep='',collapse='');
fstat<-parse_fstat(readLines(fstat_url));
fstat$m<- paste("0",fstat$month_nu,sep="")
fstat$m<- substrRight(fstat$m,2)
fstat$d<- paste("0",fstat$day_nu,sep="")
fstat$d<- substrRight(fstat$d,2)
fstat$md<- paste(fstat$m,"-",fstat$d,sep="")
flow$md<- substrRight(flow$datetime,5)
fstat<-fstat[which(fstat$parameter_cd==parameterCd),]
n<-dim(flow)[1]
flow<- flow[2:n,]
colnames(flow)[4]<-"q"
flowstat<- merge(flow,fstat,by="md")
flowstat<- transform(flowstat,q=as.numeric(q),max_va=as.numeric(max_va),min_va=as.numeric(min_va),
p10_va=as.numeric(p10_va),p25_va=as.numeric(p25_va),p75_va=as.numeric(p75_va),
p90_va=as.numeric(p90_va))
# The seven percentile classes are defined as follows:
#
# 1 = Lowest ever for the date at a streamgage
# 2 = < 10th percentile
# 3 = 10th to 24th percentile
# 4 = 25th to 75th percentile
# 5 = 76th to 90th percentile
# 6 = > 90th percentile
# 7 = Highest ever for the date at a streamgage
flowstat$index<- ifelse(flowstat$q==flowstat$min_va,1,
ifelse(flowstat$q<=flowstat$p10_va &
flowstat$q>flowstat$min_va,2,
ifelse(flowstat$q>flowstat$p10_va &
flowstat$q<=flowstat$p25_va,3,
ifelse(flowstat$q>flowstat$p25_va &
flowstat$q<=flowstat$p75,4,
ifelse(flowstat$q>flowstat$p75 &
flowstat$q<=flowstat$p90,5,
ifelse(flowstat$q>flowstat$p90 &
flowstat$q<flowstat$max_va,6,
ifelse(flowstat$q==flowstat$max_va,7,NA)))))))
sfindex<-data.frame(flowstat$datetime,flowstat$index)
colnames(sfindex)<-c("datetime",site)
sfindex #return the sfindex
}
#reused parameters here
base_url    <-'https://waterservices.usgs.gov/nwis';
start_date  <-'1997-01-01';
end_date    <-'2017-05-01';
parameterCd <-'00060';
D<-list(); #our empty list of list data sructure
n<-2; #used for limiting test range in a functional loop
for(i in 1:dim(indexgage)[1]){#read out all the individual data tables
site<-indexgage[i,1]
sfindex<-get_sfindex(site,base_url,start_date,end_date,parameterCd);
for(j in 1:dim(sfindex)[1]){ #read through the rows and load data into D
d<-as.character(sfindex[j,'datetime']);
v<-sfindex[j,site];
if(is.null(D[[d]])){ #d is not in D =>(add a new datetime key and a new key site and index value v)
D[[d]]<-list();
D[[d]][[site]]<-v;
}else{               #d is in D =>(add a new key site and index value v)
D[[d]][[site]]<-v;
}
}
}
#[1]build a new empty dataframe E
n<-length(D);
m<- dim(indexgage)[1];
E<-matrix(NA,nrow=n,ncol=m);
dates<-sort(names(D))
colnames(E)<-indexgage[,1];
rownames(E)<-dates;
#[2]read from the D and write into E
for(i in 1:n){#for each datetime
sites <- names(D[[dates[i]]]); #sites in that datetime
for(j in 1:length(D[[dates[i]]])){#for each site
E[i,sites[j]]<-D[[dates[i]]][[j]];
}
}
findex<-as.data.frame(E)
findex$index<-rowMeans(findex,na.rm=TRUE)     #only run if first time run, if index exists will include in avg
findex$gagecnt<-(rowSums(!is.na(findex)))-1  #only run -1 if first time run, if gagecnt already exists than -2
findex$sdate<- row.names(findex)
findex$sdate<- ymd(findex$sdate)
findexsummer2016<-findex[which(findex$sdate>='2016-06-01'&findex$sdate<'2016-09-01'),]
findexsummer2016lg<-findexsummer2016[,c(1:13,15)]
findexsummer2016lg<-melt(findexsummer2016lg,id=c("sdate","index"))
findex$syear<- substr(findex$sdate,1,4)
findex$smonth<- substr(findex$sdate,6,7)
findexsummer<-findex[which(findex$smonth=='06'|findex$smonth=='07'|findex$smonth=='08'),]
findexsummeravg<-aggregate(findexsummer$index,list(findexsummer$syear),mean)
colnames(findexsummeravg)<-c("Year","Index")
ggplot(findexsummer2016,aes(sdate,index))+
geom_line(colour="red",size=1.5)+
labs(y="Average streamflow index",x="2016",title="Least Disturbed Flow Index Summer 2016")+
scale_y_continuous(limits=c(1,7),breaks=c(1,2,3,4,5,6,7),labels=c("Dry  1",2,3,"Normal  4",5,6,"Wet  7"))+
theme_light()
findexsummer2016[1:10,]
findexsummeravggage<-colMeans(findexsummer2016,na.rm=TRUE)
str(findexsummer2016)
findexsummeravggage<-colMeans(findexsummer2016[,1:12],na.rm=TRUE)
findexsummeravggage
findexsummeravggage<-melt(findexsummeravggage)
findexsummeravggage
ggplot(findexsummeravggage,aes(value))+
geom_bar(stat="identity")+
labs(y="Average streamflow Index",title="Average summer (June - August) least disturbed streamflow index")+
theme_light()
findexsummeravggage$gage<-row.names(findexsummeravggage)
findexsummeravggage
ggplot(findexsummeravggage,aes(gage,value))+
geom_bar(stat="identity")+
labs(y="Average streamflow Index",title="Average summer (June - August) least disturbed streamflow index")+
theme_light()
